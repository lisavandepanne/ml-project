{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.helpers import *\n",
    "from utilities.Data_preprocessing_global import *\n",
    "from implementations import *\n",
    "from utilities.Hyperparameters_Logreg import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have to change the path for it to work\n",
    "#data_path = r'C:\\Users\\natha\\Documents\\EPFL\\Cours_MA1\\ML\\ML_course\\projects\\project1 - withGit\\data\\dataset'\n",
    "data_path = \"D:\\\\EPFL\\\\MA1\\\\Machine Learning\\\\Projet 1\\\\dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids, headers_train = load_csv_data(data_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See the different shapes : x_train (328135, 321), x_test (109379, 321), y_train (328135,), headers_train: 321\n",
      "After preprocessing (train) : column with missing values {}, are there NaN ? False\n",
      "After preprocessing (test) : column with missing values {}, are there NaN ? False\n",
      "See the different shapes : x_tr (328135, 169), x_val (0, 169), y_tr (328135,), y_te(0,), x_test_formatted(109379, 169)\n"
     ]
    }
   ],
   "source": [
    "x_tr, x_val, y_tr, y_val, x_train_full, x_test_formatted, remaining_headers = data_preprocess(x_train, y_train, x_test, headers_train, model_labels = {0, 1}, ratio_miss = 0.1, ratio_train = 1, standardization = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We intentionally select a split training vs validation of 100% vs 0% because this split will be done inside the K-Fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are defining 3 hyperparameters, the missing ratio of data acceptable in each column, the regularization term, the learning rate and the limit\n",
    "## to discriminate the predictions to -1 or 1\n",
    "\n",
    "# Here I only have a small list of parameter to verify that it all works correctly, feel free to adapt them\n",
    "k_fold = 3\n",
    "regularization_term = [0.01, 0.001]\n",
    "learning_rate = [0.1, 0.01, 0.001]\n",
    "limits = np.linspace(0.2,0.5,4)\n",
    "\n",
    "initial_w = np.zeros(x_tr.shape[1])\n",
    "max_iters = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Hyperparameters(x_tr, y_tr, k_fold, max_iters, regularization_term, learning_rate, limits, initial_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_best_results(results):\n",
    "    # Initialize variables to track the best F1-score and corresponding parameters\n",
    "    best_f1_score = 0\n",
    "    best_params = None\n",
    "\n",
    "    # Loop through the results to find the best F1-score on the validation set\n",
    "    for result in results:\n",
    "        f1_score_val = result[\"av_f1_score\"]\n",
    "\n",
    "        # Update if a new best F1-score is found\n",
    "        if f1_score_val > best_f1_score:\n",
    "            best_f1_score = f1_score_val\n",
    "            best_params = {\n",
    "                \"regularization_term\": result[\"regularization_term\"],\n",
    "                \"learning_rate\": result[\"learning_rate\"],\n",
    "                \"limit\": result[\"limit\"],\n",
    "            }\n",
    "\n",
    "    # Output the best F1-score and corresponding parameters\n",
    "    print(\"Best F1-Score:\", best_f1_score)\n",
    "    print(\"Best Parameters and Metrics:\", best_params)\n",
    "    return best_f1_score, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1-Score: 0.1764537250361379\n",
      "Best Parameters and Metrics: {'regularization_term': 0.001, 'learning_rate': 0.1, 'limit': np.float64(0.2)}\n"
     ]
    }
   ],
   "source": [
    "best_f1_score, best_params = Get_best_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have found the best hyperparameters, we can use them to train our model one last time, on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805599446\n",
      "Current iteration=100, loss=0.23226847566932823\n",
      "Current iteration=200, loss=0.22838252971242326\n",
      "Current iteration=300, loss=0.2268618563995702\n",
      "Current iteration=400, loss=0.2259811869250319\n",
      "Current iteration=500, loss=0.22538842717410226\n",
      "Current iteration=600, loss=0.22495506215754574\n",
      "Current iteration=700, loss=0.22462094544609915\n",
      "Current iteration=800, loss=0.22435362956031\n",
      "Current iteration=900, loss=0.2241339002152805\n",
      "loss=0.22394958817311608\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.001\n",
    "initial_w = np.zeros(x_tr.shape[1])\n",
    "max_iters = 1000\n",
    "gamma = 0.1\n",
    "\n",
    "w_opt, loss_opt = reg_logistic_regression(y_tr, x_tr, lambda_, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9145016532829475\n",
      "0.17555614328954713\n"
     ]
    }
   ],
   "source": [
    "y_pred, accuracy, precision, recall, f1_score = evaluate_performance(x_tr, y_tr, w_opt, {0,1}, 0.2)\n",
    "print(accuracy)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_limit = 0.2\n",
    "best_w = w_opt\n",
    "name = 'best_log_reg.csv'\n",
    "model_labels = {0, 1}\n",
    "\n",
    "y_pred_test = submission(x_test_formatted, test_ids, best_limit, best_w, name, model_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
