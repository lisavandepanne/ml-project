{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is a powerful method if we want to perform a classification task. Given that we have many datapoints compared to the number of features, solving the primal problem is more appropriate.\n",
    "\n",
    "In addition, we added some modification to the usual SVM method: during the training, we penalize more strongly errors regarding the label \"+1\" as it appears way less frequently than the \"-1\". This is done to push our model to have better F1-score (a metric that takes into account the unequal distribution of \"+1\" and \"-1\" labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities.helpers import *\n",
    "from utilities.Data_preprocessing_global import *\n",
    "from utilities.Hyperparameters_SVM import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have to change the path for it to work\n",
    "data_path = r'C:\\Users\\natha\\Documents\\EPFL\\Cours_MA1\\ML\\ML_course\\projects\\project1 - withGit\\data\\dataset'\n",
    "#data_path = \"D:\\\\EPFL\\\\MA1\\\\Machine Learning\\\\Projet 1\\\\dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids, headers_train = load_csv_data(data_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See the different shapes : x_train (328135, 321), x_test (109379, 321), y_train (328135,), headers_train: 321\n",
      "After preprocessing (train) : column with missing values {}, are there NaN ? False\n",
      "After preprocessing (test) : column with missing values {}, are there NaN ? False\n",
      "See the different shapes : x_tr (328135, 169), x_val (0, 169), y_tr (328135,), y_te(0,), x_test_formatted(109379, 169)\n"
     ]
    }
   ],
   "source": [
    "x_tr, x_val, y_tr, y_val, x_train_full, x_test_formatted, remaining_headers = data_preprocess(x_train, y_train, x_test, headers_train, model_labels = {-1, 1}, ratio_miss = 0.1, ratio_train = 1, standardization = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We intentionally select a split training vs validation of 100% vs 0% because this split will be done inside the K-Fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We increase the iteration with gamma, because we observed in our testing that as the gamma decreased the iteration to converge were higher. And as we did not want to iterate for no significant improvement for the higher step size, we add that the maximum number of iteration is dependant on the gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters we choose to test\n",
    "k_fold = 5\n",
    "penalty_factor_list = [8, 9, 10, 11, 12, 13]\n",
    "gamma_list = [0.1, 0.01]\n",
    "lambda_list = [0.1, 0.01, 0.001]\n",
    "a_list = [0.5, 0.75] # Power of the denominator in the decreasing step size\n",
    "\n",
    "results = Hyperparameter(x_tr, y_tr, penalty_factor_list, gamma_list, lambda_list, a_list, k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the information from our grid-search + K-Fold in a csv file, to later analyse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Specify the file path where you want to save the CSV file\n",
    "file_path = 'results.csv'\n",
    "\n",
    "# Write the list of dictionaries to the CSV file\n",
    "with open(file_path, 'w', newline='') as csv_file:\n",
    "    fieldnames = results[0].keys()  # Get the field names from the first dictionary\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()  # Write the header\n",
    "    writer.writerows(results)  # Write the data\n",
    "\n",
    "print(f\"Results saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1-Score: 0.3502768799667791\n",
      "Best Parameters and Metrics: {'penalty_factor': 10, 'gamma': 0.01, 'lambda': 0.01, 'a': 0.5}\n"
     ]
    }
   ],
   "source": [
    "best_f1_score, best_params = Get_best_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have found the best hyperparameters, we can use them to train our model one last time, on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_factor = 10\n",
    "gamma = 0.01\n",
    "lambda_ = 0.01\n",
    "a = 0.5\n",
    "max_iter = int(10 / gamma)\n",
    "\n",
    "w_opt, loss_tr = sgd_for_svm(y_tr, x_tr, max_iter, gamma, lambda_, a, penalty_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate performance on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7875112377527542\n",
      "0.34757792104499813\n"
     ]
    }
   ],
   "source": [
    "y_pred, accuracy, precision, recall, f1_score = evaluate_performance(x_tr, y_tr, w_opt, model_labels={-1, 1}, limit=0)\n",
    "print(accuracy)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_limit = 0 #It doesn't matter here, if we say that the model_labels are {-1, 1}\n",
    "best_w = w_opt\n",
    "name = 'best_SVM_submission_file.csv'\n",
    "model_labels = {-1, 1}\n",
    "\n",
    "y_pred_test = submission(x_test_formatted, test_ids, best_limit, best_w, name, model_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: comparison between having a penalty factor and using the regular weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the difference, we make a split training vs validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See the different shapes : x_train (328135, 321), x_test (109379, 321), y_train (328135,), headers_train: 321\n",
      "After preprocessing (train) : column with missing values {}, are there NaN ? False\n",
      "After preprocessing (test) : column with missing values {}, are there NaN ? False\n",
      "See the different shapes : x_tr (262508, 169), x_val (65627, 169), y_tr (262508,), y_te(65627,), x_test_formatted(109379, 169)\n"
     ]
    }
   ],
   "source": [
    "x_tr, x_val, y_tr, y_val, x_train_full, x_test_formatted, remaining_headers = data_preprocess(x_train, y_train, x_test, headers_train, model_labels = {-1, 1}, ratio_miss = 0.1, ratio_train = 0.8, standardization = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With higher penalization of +1 misclassification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.01\n",
    "max_iter = int(10 / gamma)\n",
    "lambda_ = 0.01\n",
    "a = 0.5\n",
    "penalty_factor = 10\n",
    "\n",
    "w_opt, loss_tr = sgd_for_svm(y_tr, x_tr, max_iter, gamma, lambda_, a, penalty_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34311801689960353\n"
     ]
    }
   ],
   "source": [
    "y_pred, accuracy, precision, recall, f1_score = evaluate_performance(x_val, y_val, w_opt, model_labels={-1, 1}, limit=0)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regular penalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.01\n",
    "max_iter = int(10 / gamma)\n",
    "lambda_ = 0.001\n",
    "a = 0.1\n",
    "penalty_factor = 1\n",
    "\n",
    "w_opt, loss_tr = sgd_for_svm(y_tr, x_tr, max_iter, gamma, lambda_, a, penalty_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15450264861683344\n"
     ]
    }
   ],
   "source": [
    "y_pred, accuracy, precision, recall, f1_score = evaluate_performance(x_val, y_val, w_opt, model_labels={-1, 1}, limit=0)\n",
    "print(f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
