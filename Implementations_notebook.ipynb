{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities.helpers import *\n",
    "from utilities.Data_preprocessing_global import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have to change the path for it to work\n",
    "data_path = r'C:\\Users\\natha\\Documents\\EPFL\\Cours_MA1\\ML\\ML_course\\projects\\project1 - withGit\\data\\dataset'\n",
    "#data_path = \"D:\\\\EPFL\\\\MA1\\\\Machine Learning\\\\Projet 1\\\\dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids, headers_train = load_csv_data(data_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See the different shapes : x_train (328135, 321), x_test (109379, 321), y_train (328135,), headers_train: 321\n",
      "After preprocessing (train) : column with missing values {}, are there NaN ? False\n",
      "After preprocessing (test) : column with missing values {}, are there NaN ? False\n",
      "See the different shapes : x_tr (262508, 169), x_val (65627, 169), y_tr (262508,), y_te(65627,), x_test_formatted(109379, 169)\n"
     ]
    }
   ],
   "source": [
    "# By selecting ratio_miss < 1, after the first selection of features (see pre-processing file) we keep all the columns\n",
    "x_tr, x_val, y_tr, y_val, x_train_full, x_test_formatted, remaining_headers = data_preprocess(x_train, y_train, x_test, headers_train, model_labels = {-1, 1}, ratio_miss = 0.1, ratio_train = 0.8, standardization = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the basic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Linear regression using gradient descent : training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/99: loss=0.5\n",
      "GD iter. 1/99: loss=0.30137290245451986\n",
      "GD iter. 2/99: loss=0.21881046355990472\n",
      "GD iter. 3/99: loss=0.18353151355235583\n",
      "GD iter. 4/99: loss=0.16767463426767437\n",
      "GD iter. 5/99: loss=0.1599238930609318\n",
      "GD iter. 6/99: loss=0.15565929668244882\n",
      "GD iter. 7/99: loss=0.15297542562154454\n",
      "GD iter. 8/99: loss=0.15107113785734497\n",
      "GD iter. 9/99: loss=0.14959751122185233\n",
      "GD iter. 10/99: loss=0.14839320129719508\n",
      "GD iter. 11/99: loss=0.14737655445697365\n",
      "GD iter. 12/99: loss=0.14650124458910527\n",
      "GD iter. 13/99: loss=0.145737729852573\n",
      "GD iter. 14/99: loss=0.1450652658436913\n",
      "GD iter. 15/99: loss=0.14446828334665524\n",
      "GD iter. 16/99: loss=0.14393460889295923\n",
      "GD iter. 17/99: loss=0.14345449315899603\n",
      "GD iter. 18/99: loss=0.14302001619815352\n",
      "GD iter. 19/99: loss=0.14262468513959206\n",
      "GD iter. 20/99: loss=0.14226314172288468\n",
      "GD iter. 21/99: loss=0.141930939805732\n",
      "GD iter. 22/99: loss=0.14162437158410543\n",
      "GD iter. 23/99: loss=0.14134032983112088\n",
      "GD iter. 24/99: loss=0.14107619775755925\n",
      "GD iter. 25/99: loss=0.14082976049680193\n",
      "GD iter. 26/99: loss=0.1405991337143648\n",
      "GD iter. 27/99: loss=0.14038270586697077\n",
      "GD iter. 28/99: loss=0.14017909138409995\n",
      "GD iter. 29/99: loss=0.13998709261301231\n",
      "GD iter. 30/99: loss=0.1398056688094606\n",
      "GD iter. 31/99: loss=0.13963391080328305\n",
      "GD iter. 32/99: loss=0.1394710202427635\n",
      "GD iter. 33/99: loss=0.13931629253994862\n",
      "GD iter. 34/99: loss=0.1391691028129941\n",
      "GD iter. 35/99: loss=0.1390288942603235\n",
      "GD iter. 36/99: loss=0.1388951685121653\n",
      "GD iter. 37/99: loss=0.13876747759360633\n",
      "GD iter. 38/99: loss=0.13864541720417733\n",
      "GD iter. 39/99: loss=0.13852862107576536\n",
      "GD iter. 40/99: loss=0.13841675621617858\n",
      "GD iter. 41/99: loss=0.13830951888223791\n",
      "GD iter. 42/99: loss=0.13820663115564488\n",
      "GD iter. 43/99: loss=0.1381078380185148\n",
      "GD iter. 44/99: loss=0.13801290484451012\n",
      "GD iter. 45/99: loss=0.13792161523688026\n",
      "GD iter. 46/99: loss=0.1378337691571346\n",
      "GD iter. 47/99: loss=0.1377491812981326\n",
      "GD iter. 48/99: loss=0.1376676796635274\n",
      "GD iter. 49/99: loss=0.13758910432212523\n",
      "GD iter. 50/99: loss=0.13751330631111494\n",
      "GD iter. 51/99: loss=0.13744014666652177\n",
      "GD iter. 52/99: loss=0.1373694955628369\n",
      "GD iter. 53/99: loss=0.13730123154672003\n",
      "GD iter. 54/99: loss=0.13723524085209657\n",
      "GD iter. 55/99: loss=0.13717141678596317\n",
      "GD iter. 56/99: loss=0.1371096591758656\n",
      "GD iter. 57/99: loss=0.1370498738713751\n",
      "GD iter. 58/99: loss=0.13699197229302598\n",
      "GD iter. 59/99: loss=0.13693587102312002\n",
      "GD iter. 60/99: loss=0.1368814914335943\n",
      "GD iter. 61/99: loss=0.1368287593468118\n",
      "GD iter. 62/99: loss=0.13677760472569242\n",
      "GD iter. 63/99: loss=0.13672796139007146\n",
      "GD iter. 64/99: loss=0.13667976675657315\n",
      "GD iter. 65/99: loss=0.1366329615996275\n",
      "GD iter. 66/99: loss=0.13658748983154453\n",
      "GD iter. 67/99: loss=0.1365432982998133\n",
      "GD iter. 68/99: loss=0.13650033660000252\n",
      "GD iter. 69/99: loss=0.13645855690282563\n",
      "GD iter. 70/99: loss=0.136417913794092\n",
      "GD iter. 71/99: loss=0.13637836412640406\n",
      "GD iter. 72/99: loss=0.13633986688158145\n",
      "GD iter. 73/99: loss=0.1363023830428982\n",
      "GD iter. 74/99: loss=0.13626587547631153\n",
      "GD iter. 75/99: loss=0.1362303088199424\n",
      "GD iter. 76/99: loss=0.13619564938114026\n",
      "GD iter. 77/99: loss=0.13616186504052716\n",
      "GD iter. 78/99: loss=0.1361289251624728\n",
      "GD iter. 79/99: loss=0.13609680051150383\n",
      "GD iter. 80/99: loss=0.13606546317419402\n",
      "GD iter. 81/99: loss=0.13603488648612208\n",
      "GD iter. 82/99: loss=0.13600504496352095\n",
      "GD iter. 83/99: loss=0.1359759142392735\n",
      "GD iter. 84/99: loss=0.13594747100293947\n",
      "GD iter. 85/99: loss=0.13591969294452436\n",
      "GD iter. 86/99: loss=0.13589255870172423\n",
      "GD iter. 87/99: loss=0.13586604781040287\n",
      "GD iter. 88/99: loss=0.1358401406580771\n",
      "GD iter. 89/99: loss=0.13581481844020177\n",
      "GD iter. 90/99: loss=0.13579006311906547\n",
      "GD iter. 91/99: loss=0.13576585738511954\n",
      "GD iter. 92/99: loss=0.13574218462057822\n",
      "GD iter. 93/99: loss=0.13571902886513929\n",
      "GD iter. 94/99: loss=0.13569637478368543\n",
      "GD iter. 95/99: loss=0.1356742076358386\n",
      "GD iter. 96/99: loss=0.1356525132472456\n",
      "GD iter. 97/99: loss=0.13563127798248584\n",
      "GD iter. 98/99: loss=0.13561048871949744\n",
      "GD iter. 99/99: loss=0.135590132825425\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(x_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "\n",
    "w_opt, loss_opt = mean_squared_error_gd(y_tr, x_tr, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Linear regression using gradient descent : performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1-score:  0.02977667493796526\n",
      "\n",
      "Validation F1-score:  0.03219106957424714\n"
     ]
    }
   ],
   "source": [
    "y_pred, accuracy, precision, recall, f1_score = evaluate_performance(x_tr, y_tr, w_opt, model_labels={-1, 1}, limit=0)\n",
    "print(\"Training F1-score: \", f1_score)\n",
    "y_pred, accuracy, precision, recall, f1_score = evaluate_performance(x_val, y_val, w_opt, model_labels={-1, 1}, limit=0)\n",
    "print(\"\\nValidation F1-score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Linear regression using SGD : training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/999: loss=0.5\n",
      "SGD iter. 1/999: loss=0.47316522037998465\n",
      "SGD iter. 2/999: loss=0.443521234123469\n",
      "SGD iter. 3/999: loss=0.4177766711502656\n",
      "SGD iter. 4/999: loss=0.3947123172487016\n",
      "SGD iter. 5/999: loss=0.377637510437788\n",
      "SGD iter. 6/999: loss=0.35778204651572615\n",
      "SGD iter. 7/999: loss=0.33934459244836307\n",
      "SGD iter. 8/999: loss=0.3227884743897791\n",
      "SGD iter. 9/999: loss=0.3071910912534793\n",
      "SGD iter. 10/999: loss=0.29474193655679276\n",
      "SGD iter. 11/999: loss=0.28203215278015986\n",
      "SGD iter. 12/999: loss=0.2712713314138531\n",
      "SGD iter. 13/999: loss=0.26263412711277884\n",
      "SGD iter. 14/999: loss=0.25334540535669614\n",
      "SGD iter. 15/999: loss=0.24566623999002013\n",
      "SGD iter. 16/999: loss=0.26448118099587664\n",
      "SGD iter. 17/999: loss=0.25481075162168915\n",
      "SGD iter. 18/999: loss=0.24617050087499803\n",
      "SGD iter. 19/999: loss=0.23783623426902267\n",
      "SGD iter. 20/999: loss=0.23039084127473583\n",
      "SGD iter. 21/999: loss=0.22400862963256032\n",
      "SGD iter. 22/999: loss=0.21504540939828562\n",
      "SGD iter. 23/999: loss=0.21013305438550164\n",
      "SGD iter. 24/999: loss=0.20573676423111806\n",
      "SGD iter. 25/999: loss=0.20163290466976613\n",
      "SGD iter. 26/999: loss=0.19702192944854352\n",
      "SGD iter. 27/999: loss=0.19430149250845655\n",
      "SGD iter. 28/999: loss=0.19123090031168655\n",
      "SGD iter. 29/999: loss=0.18775811594337888\n",
      "SGD iter. 30/999: loss=0.20521123515617742\n",
      "SGD iter. 31/999: loss=0.20078570040215807\n",
      "SGD iter. 32/999: loss=0.19731077337750255\n",
      "SGD iter. 33/999: loss=0.19308822568925918\n",
      "SGD iter. 34/999: loss=0.1890012195230587\n",
      "SGD iter. 35/999: loss=0.20205886682480478\n",
      "SGD iter. 36/999: loss=0.19779236792547203\n",
      "SGD iter. 37/999: loss=0.1942065796283235\n",
      "SGD iter. 38/999: loss=0.19112332035747012\n",
      "SGD iter. 39/999: loss=0.187929620959301\n",
      "SGD iter. 40/999: loss=0.19416412985589476\n",
      "SGD iter. 41/999: loss=0.19075952007989114\n",
      "SGD iter. 42/999: loss=0.18760982626153674\n",
      "SGD iter. 43/999: loss=0.18457079580901056\n",
      "SGD iter. 44/999: loss=0.18302242759669948\n",
      "SGD iter. 45/999: loss=0.18053156138800472\n",
      "SGD iter. 46/999: loss=0.1783359608486716\n",
      "SGD iter. 47/999: loss=0.1879166258587237\n",
      "SGD iter. 48/999: loss=0.18559813871532532\n",
      "SGD iter. 49/999: loss=0.1829209867888968\n",
      "SGD iter. 50/999: loss=0.18132566224907062\n",
      "SGD iter. 51/999: loss=0.17949906744486874\n",
      "SGD iter. 52/999: loss=0.17782510998130147\n",
      "SGD iter. 53/999: loss=0.1767463112061283\n",
      "SGD iter. 54/999: loss=0.17595784276502557\n",
      "SGD iter. 55/999: loss=0.17508999369209113\n",
      "SGD iter. 56/999: loss=0.17408541536127173\n",
      "SGD iter. 57/999: loss=0.17299127614463142\n",
      "SGD iter. 58/999: loss=0.17228571365245005\n",
      "SGD iter. 59/999: loss=0.17176362636288547\n",
      "SGD iter. 60/999: loss=0.1686795050686619\n",
      "SGD iter. 61/999: loss=0.16938954079513036\n",
      "SGD iter. 62/999: loss=0.16810814680771685\n",
      "SGD iter. 63/999: loss=0.1656972014063949\n",
      "SGD iter. 64/999: loss=0.16351062968617736\n",
      "SGD iter. 65/999: loss=0.16927984848286964\n",
      "SGD iter. 66/999: loss=0.16851733661509619\n",
      "SGD iter. 67/999: loss=0.1676326577609609\n",
      "SGD iter. 68/999: loss=0.1641708331154863\n",
      "SGD iter. 69/999: loss=0.1630544633555495\n",
      "SGD iter. 70/999: loss=0.16204174253840176\n",
      "SGD iter. 71/999: loss=0.16089529369550137\n",
      "SGD iter. 72/999: loss=0.15971785207441067\n",
      "SGD iter. 73/999: loss=0.15782046136074054\n",
      "SGD iter. 74/999: loss=0.15763871606766403\n",
      "SGD iter. 75/999: loss=0.15643230411196657\n",
      "SGD iter. 76/999: loss=0.1548942683485332\n",
      "SGD iter. 77/999: loss=0.1539952078329512\n",
      "SGD iter. 78/999: loss=0.15375221482705953\n",
      "SGD iter. 79/999: loss=0.15390381409318846\n",
      "SGD iter. 80/999: loss=0.16358848064179374\n",
      "SGD iter. 81/999: loss=0.16140106135224588\n",
      "SGD iter. 82/999: loss=0.1601953166270543\n",
      "SGD iter. 83/999: loss=0.15942039388982965\n",
      "SGD iter. 84/999: loss=0.15833863542367582\n",
      "SGD iter. 85/999: loss=0.15712562476305642\n",
      "SGD iter. 86/999: loss=0.15561610423667152\n",
      "SGD iter. 87/999: loss=0.15484105706917295\n",
      "SGD iter. 88/999: loss=0.15399536220833365\n",
      "SGD iter. 89/999: loss=0.15927815027990513\n",
      "SGD iter. 90/999: loss=0.15674547414376971\n",
      "SGD iter. 91/999: loss=0.15563451886893614\n",
      "SGD iter. 92/999: loss=0.15505672202700957\n",
      "SGD iter. 93/999: loss=0.15460480868905038\n",
      "SGD iter. 94/999: loss=0.1541971653381132\n",
      "SGD iter. 95/999: loss=0.15290831070544705\n",
      "SGD iter. 96/999: loss=0.1521269360661458\n",
      "SGD iter. 97/999: loss=0.1519726752576071\n",
      "SGD iter. 98/999: loss=0.15173285654840463\n",
      "SGD iter. 99/999: loss=0.15171225776080866\n",
      "SGD iter. 100/999: loss=0.1509398148046843\n",
      "SGD iter. 101/999: loss=0.1509881583880483\n",
      "SGD iter. 102/999: loss=0.1509919570032835\n",
      "SGD iter. 103/999: loss=0.15098533438380807\n",
      "SGD iter. 104/999: loss=0.15112038235566783\n",
      "SGD iter. 105/999: loss=0.15055287263150016\n",
      "SGD iter. 106/999: loss=0.15080073183583806\n",
      "SGD iter. 107/999: loss=0.15081374279466503\n",
      "SGD iter. 108/999: loss=0.15047158552761794\n",
      "SGD iter. 109/999: loss=0.15084107545089037\n",
      "SGD iter. 110/999: loss=0.15107875062429502\n",
      "SGD iter. 111/999: loss=0.1511538613343709\n",
      "SGD iter. 112/999: loss=0.15115053516175883\n",
      "SGD iter. 113/999: loss=0.15163075967246983\n",
      "SGD iter. 114/999: loss=0.15164331697345068\n",
      "SGD iter. 115/999: loss=0.15152374923879008\n",
      "SGD iter. 116/999: loss=0.15154596963420439\n",
      "SGD iter. 117/999: loss=0.15103461016393452\n",
      "SGD iter. 118/999: loss=0.15152042515283054\n",
      "SGD iter. 119/999: loss=0.1484532947431154\n",
      "SGD iter. 120/999: loss=0.14847033127228618\n",
      "SGD iter. 121/999: loss=0.1484635137853203\n",
      "SGD iter. 122/999: loss=0.14883175707427015\n",
      "SGD iter. 123/999: loss=0.15161733370447067\n",
      "SGD iter. 124/999: loss=0.1514084662992882\n",
      "SGD iter. 125/999: loss=0.1508735592357209\n",
      "SGD iter. 126/999: loss=0.15092566613244326\n",
      "SGD iter. 127/999: loss=0.1502859941676359\n",
      "SGD iter. 128/999: loss=0.1552234910715563\n",
      "SGD iter. 129/999: loss=0.1547077657461284\n",
      "SGD iter. 130/999: loss=0.1545142860221726\n",
      "SGD iter. 131/999: loss=0.15409075982199175\n",
      "SGD iter. 132/999: loss=0.15415504625100335\n",
      "SGD iter. 133/999: loss=0.15387026853665733\n",
      "SGD iter. 134/999: loss=0.1538814803395\n",
      "SGD iter. 135/999: loss=0.15339143563834934\n",
      "SGD iter. 136/999: loss=0.15332182763098473\n",
      "SGD iter. 137/999: loss=0.15296325953203313\n",
      "SGD iter. 138/999: loss=0.15295582992911\n",
      "SGD iter. 139/999: loss=0.15295112133037272\n",
      "SGD iter. 140/999: loss=0.15229949038768067\n",
      "SGD iter. 141/999: loss=0.15224130414385023\n",
      "SGD iter. 142/999: loss=0.15164262966776815\n",
      "SGD iter. 143/999: loss=0.15152024017326218\n",
      "SGD iter. 144/999: loss=0.15070847240363977\n",
      "SGD iter. 145/999: loss=0.15060647924471593\n",
      "SGD iter. 146/999: loss=0.15056050841156082\n",
      "SGD iter. 147/999: loss=0.15052326993995163\n",
      "SGD iter. 148/999: loss=0.15043069789260166\n",
      "SGD iter. 149/999: loss=0.15006933751795537\n",
      "SGD iter. 150/999: loss=0.15009265672849603\n",
      "SGD iter. 151/999: loss=0.15000477502209572\n",
      "SGD iter. 152/999: loss=0.15049564139126914\n",
      "SGD iter. 153/999: loss=0.15075280729026966\n",
      "SGD iter. 154/999: loss=0.15048683802402188\n",
      "SGD iter. 155/999: loss=0.1503553145947864\n",
      "SGD iter. 156/999: loss=0.15037913571272885\n",
      "SGD iter. 157/999: loss=0.1502917181986991\n",
      "SGD iter. 158/999: loss=0.15037120372533283\n",
      "SGD iter. 159/999: loss=0.1499938610757744\n",
      "SGD iter. 160/999: loss=0.15012046339263393\n",
      "SGD iter. 161/999: loss=0.14896839213732269\n",
      "SGD iter. 162/999: loss=0.14897324656704125\n",
      "SGD iter. 163/999: loss=0.1507681364095504\n",
      "SGD iter. 164/999: loss=0.1511701178044117\n",
      "SGD iter. 165/999: loss=0.1510042623448084\n",
      "SGD iter. 166/999: loss=0.15221300632186446\n",
      "SGD iter. 167/999: loss=0.15376925422426485\n",
      "SGD iter. 168/999: loss=0.15357119058781524\n",
      "SGD iter. 169/999: loss=0.15302840058856243\n",
      "SGD iter. 170/999: loss=0.15268527853459557\n",
      "SGD iter. 171/999: loss=0.1531220049940274\n",
      "SGD iter. 172/999: loss=0.1531530220274923\n",
      "SGD iter. 173/999: loss=0.15505603151059907\n",
      "SGD iter. 174/999: loss=0.15448600236774418\n",
      "SGD iter. 175/999: loss=0.1540409910310952\n",
      "SGD iter. 176/999: loss=0.15370856128041863\n",
      "SGD iter. 177/999: loss=0.15382809288696364\n",
      "SGD iter. 178/999: loss=0.1543127550141438\n",
      "SGD iter. 179/999: loss=0.15395733866067257\n",
      "SGD iter. 180/999: loss=0.15420389666981033\n",
      "SGD iter. 181/999: loss=0.1559923615243373\n",
      "SGD iter. 182/999: loss=0.15599246077545564\n",
      "SGD iter. 183/999: loss=0.15593124164426458\n",
      "SGD iter. 184/999: loss=0.1561849337238526\n",
      "SGD iter. 185/999: loss=0.14880782197480585\n",
      "SGD iter. 186/999: loss=0.1467889450994409\n",
      "SGD iter. 187/999: loss=0.14700831734461106\n",
      "SGD iter. 188/999: loss=0.1470178978208585\n",
      "SGD iter. 189/999: loss=0.14671405011848165\n",
      "SGD iter. 190/999: loss=0.1467855773133992\n",
      "SGD iter. 191/999: loss=0.1468784567952509\n",
      "SGD iter. 192/999: loss=0.14711480054820955\n",
      "SGD iter. 193/999: loss=0.14787656200037233\n",
      "SGD iter. 194/999: loss=0.14774957814532697\n",
      "SGD iter. 195/999: loss=0.14750413958166894\n",
      "SGD iter. 196/999: loss=0.14719802000048562\n",
      "SGD iter. 197/999: loss=0.1470336204455389\n",
      "SGD iter. 198/999: loss=0.14927285973039964\n",
      "SGD iter. 199/999: loss=0.1492112357592596\n",
      "SGD iter. 200/999: loss=0.14913703580716914\n",
      "SGD iter. 201/999: loss=0.14911031102004246\n",
      "SGD iter. 202/999: loss=0.14859322035414213\n",
      "SGD iter. 203/999: loss=0.1485394127555425\n",
      "SGD iter. 204/999: loss=0.14847625879613774\n",
      "SGD iter. 205/999: loss=0.14851917518275382\n",
      "SGD iter. 206/999: loss=0.14851155584168937\n",
      "SGD iter. 207/999: loss=0.14834019620924715\n",
      "SGD iter. 208/999: loss=0.14830442103395045\n",
      "SGD iter. 209/999: loss=0.14855298044883297\n",
      "SGD iter. 210/999: loss=0.14853261923587016\n",
      "SGD iter. 211/999: loss=0.14899688478963674\n",
      "SGD iter. 212/999: loss=0.14922143404156452\n",
      "SGD iter. 213/999: loss=0.14876999294798623\n",
      "SGD iter. 214/999: loss=0.14909152866321615\n",
      "SGD iter. 215/999: loss=0.14935488458701862\n",
      "SGD iter. 216/999: loss=0.14984622931457173\n",
      "SGD iter. 217/999: loss=0.14938021107380847\n",
      "SGD iter. 218/999: loss=0.14928280363309065\n",
      "SGD iter. 219/999: loss=0.1500388570916024\n",
      "SGD iter. 220/999: loss=0.15120999396203377\n",
      "SGD iter. 221/999: loss=0.15166303518559948\n",
      "SGD iter. 222/999: loss=0.1519745402636638\n",
      "SGD iter. 223/999: loss=0.15109494339548502\n",
      "SGD iter. 224/999: loss=0.1504449328511386\n",
      "SGD iter. 225/999: loss=0.1507070747598494\n",
      "SGD iter. 226/999: loss=0.15071661450245744\n",
      "SGD iter. 227/999: loss=0.15074869311821693\n",
      "SGD iter. 228/999: loss=0.15086570828702126\n",
      "SGD iter. 229/999: loss=0.15115906816894284\n",
      "SGD iter. 230/999: loss=0.15052609495357178\n",
      "SGD iter. 231/999: loss=0.15065336975076185\n",
      "SGD iter. 232/999: loss=0.1473890589577378\n",
      "SGD iter. 233/999: loss=0.14747245148561774\n",
      "SGD iter. 234/999: loss=0.14759934108054584\n",
      "SGD iter. 235/999: loss=0.1483459160273335\n",
      "SGD iter. 236/999: loss=0.14814607751551864\n",
      "SGD iter. 237/999: loss=0.14786417024186463\n",
      "SGD iter. 238/999: loss=0.1478530920179777\n",
      "SGD iter. 239/999: loss=0.14784419448264474\n",
      "SGD iter. 240/999: loss=0.1477945337716233\n",
      "SGD iter. 241/999: loss=0.14851465675321548\n",
      "SGD iter. 242/999: loss=0.14851427463936975\n",
      "SGD iter. 243/999: loss=0.1485973190949296\n",
      "SGD iter. 244/999: loss=0.14876238010342477\n",
      "SGD iter. 245/999: loss=0.14877682156988364\n",
      "SGD iter. 246/999: loss=0.1490917004532557\n",
      "SGD iter. 247/999: loss=0.1486583459089511\n",
      "SGD iter. 248/999: loss=0.14847600058777202\n",
      "SGD iter. 249/999: loss=0.14808145220768512\n",
      "SGD iter. 250/999: loss=0.14791489449295214\n",
      "SGD iter. 251/999: loss=0.14763887409749948\n",
      "SGD iter. 252/999: loss=0.14740461022775983\n",
      "SGD iter. 253/999: loss=0.147150937743378\n",
      "SGD iter. 254/999: loss=0.14768344210294626\n",
      "SGD iter. 255/999: loss=0.14754450507254296\n",
      "SGD iter. 256/999: loss=0.14743251315203437\n",
      "SGD iter. 257/999: loss=0.14767835256427778\n",
      "SGD iter. 258/999: loss=0.14763020751378497\n",
      "SGD iter. 259/999: loss=0.1478146197712776\n",
      "SGD iter. 260/999: loss=0.14780814087043365\n",
      "SGD iter. 261/999: loss=0.1478207436901112\n",
      "SGD iter. 262/999: loss=0.1478700078017494\n",
      "SGD iter. 263/999: loss=0.14792129755527542\n",
      "SGD iter. 264/999: loss=0.1504069174125236\n",
      "SGD iter. 265/999: loss=0.14931456686279337\n",
      "SGD iter. 266/999: loss=0.14933334920506686\n",
      "SGD iter. 267/999: loss=0.14943877191299051\n",
      "SGD iter. 268/999: loss=0.1494098915433006\n",
      "SGD iter. 269/999: loss=0.14934668762831843\n",
      "SGD iter. 270/999: loss=0.14935684979224695\n",
      "SGD iter. 271/999: loss=0.1493928944032723\n",
      "SGD iter. 272/999: loss=0.1535220728383386\n",
      "SGD iter. 273/999: loss=0.15291309627298763\n",
      "SGD iter. 274/999: loss=0.15948363295353293\n",
      "SGD iter. 275/999: loss=0.15877321400108896\n",
      "SGD iter. 276/999: loss=0.15894954232380995\n",
      "SGD iter. 277/999: loss=0.15698932408469343\n",
      "SGD iter. 278/999: loss=0.15503576714258982\n",
      "SGD iter. 279/999: loss=0.16085356481372337\n",
      "SGD iter. 280/999: loss=0.15883635133780136\n",
      "SGD iter. 281/999: loss=0.15945436590426315\n",
      "SGD iter. 282/999: loss=0.15903093818916547\n",
      "SGD iter. 283/999: loss=0.1579898099012145\n",
      "SGD iter. 284/999: loss=0.15778703327933852\n",
      "SGD iter. 285/999: loss=0.15766086710976684\n",
      "SGD iter. 286/999: loss=0.1635884956646076\n",
      "SGD iter. 287/999: loss=0.16339365102140277\n",
      "SGD iter. 288/999: loss=0.16085534705501192\n",
      "SGD iter. 289/999: loss=0.15944728023685664\n",
      "SGD iter. 290/999: loss=0.158669889661577\n",
      "SGD iter. 291/999: loss=0.15648912067964563\n",
      "SGD iter. 292/999: loss=0.1554649658474129\n",
      "SGD iter. 293/999: loss=0.16208301312837917\n",
      "SGD iter. 294/999: loss=0.16164075646532647\n",
      "SGD iter. 295/999: loss=0.15836138914259792\n",
      "SGD iter. 296/999: loss=0.15511641340662896\n",
      "SGD iter. 297/999: loss=0.15493248964147566\n",
      "SGD iter. 298/999: loss=0.15157347443798438\n",
      "SGD iter. 299/999: loss=0.1518041875711504\n",
      "SGD iter. 300/999: loss=0.15547012500192187\n",
      "SGD iter. 301/999: loss=0.15195335597318757\n",
      "SGD iter. 302/999: loss=0.1506432800149355\n",
      "SGD iter. 303/999: loss=0.14898567610249858\n",
      "SGD iter. 304/999: loss=0.1469383885102373\n",
      "SGD iter. 305/999: loss=0.1467607772024393\n",
      "SGD iter. 306/999: loss=0.14676236226810752\n",
      "SGD iter. 307/999: loss=0.1467988163810881\n",
      "SGD iter. 308/999: loss=0.14688303518009463\n",
      "SGD iter. 309/999: loss=0.1467194978560177\n",
      "SGD iter. 310/999: loss=0.146618407775595\n",
      "SGD iter. 311/999: loss=0.15028130289787495\n",
      "SGD iter. 312/999: loss=0.1503344136725696\n",
      "SGD iter. 313/999: loss=0.1567631945398165\n",
      "SGD iter. 314/999: loss=0.15456898243860306\n",
      "SGD iter. 315/999: loss=0.15367087639508642\n",
      "SGD iter. 316/999: loss=0.15953575134893444\n",
      "SGD iter. 317/999: loss=0.15655290045230905\n",
      "SGD iter. 318/999: loss=0.1612072289806178\n",
      "SGD iter. 319/999: loss=0.16042915305611388\n",
      "SGD iter. 320/999: loss=0.15779755363823958\n",
      "SGD iter. 321/999: loss=0.15589292334346122\n",
      "SGD iter. 322/999: loss=0.15535839266802104\n",
      "SGD iter. 323/999: loss=0.15333160651126682\n",
      "SGD iter. 324/999: loss=0.15072660268820087\n",
      "SGD iter. 325/999: loss=0.14921233308193005\n",
      "SGD iter. 326/999: loss=0.14904063105789106\n",
      "SGD iter. 327/999: loss=0.1485229528453775\n",
      "SGD iter. 328/999: loss=0.14853020549675827\n",
      "SGD iter. 329/999: loss=0.14839205649411724\n",
      "SGD iter. 330/999: loss=0.148141538019594\n",
      "SGD iter. 331/999: loss=0.15436495555361396\n",
      "SGD iter. 332/999: loss=0.15340313074236983\n",
      "SGD iter. 333/999: loss=0.15019021083311038\n",
      "SGD iter. 334/999: loss=0.14952427500276833\n",
      "SGD iter. 335/999: loss=0.149130344703993\n",
      "SGD iter. 336/999: loss=0.14919739587937259\n",
      "SGD iter. 337/999: loss=0.146663045956684\n",
      "SGD iter. 338/999: loss=0.150606423097612\n",
      "SGD iter. 339/999: loss=0.1501366505607062\n",
      "SGD iter. 340/999: loss=0.148716561803322\n",
      "SGD iter. 341/999: loss=0.1488100806997404\n",
      "SGD iter. 342/999: loss=0.1466306601842297\n",
      "SGD iter. 343/999: loss=0.14665849927760957\n",
      "SGD iter. 344/999: loss=0.14634459954450688\n",
      "SGD iter. 345/999: loss=0.1463443815142728\n",
      "SGD iter. 346/999: loss=0.14634345394611317\n",
      "SGD iter. 347/999: loss=0.1460182259189961\n",
      "SGD iter. 348/999: loss=0.1459666570859471\n",
      "SGD iter. 349/999: loss=0.14594109596378524\n",
      "SGD iter. 350/999: loss=0.1457754310561746\n",
      "SGD iter. 351/999: loss=0.1458175811256786\n",
      "SGD iter. 352/999: loss=0.1461001595787333\n",
      "SGD iter. 353/999: loss=0.14619608861578923\n",
      "SGD iter. 354/999: loss=0.14659182482883562\n",
      "SGD iter. 355/999: loss=0.14714946008421032\n",
      "SGD iter. 356/999: loss=0.14677647624976467\n",
      "SGD iter. 357/999: loss=0.14655927525624238\n",
      "SGD iter. 358/999: loss=0.14675224853062122\n",
      "SGD iter. 359/999: loss=0.1463930188192724\n",
      "SGD iter. 360/999: loss=0.1470179579534585\n",
      "SGD iter. 361/999: loss=0.14742621588482668\n",
      "SGD iter. 362/999: loss=0.14747878269603473\n",
      "SGD iter. 363/999: loss=0.1483343100379785\n",
      "SGD iter. 364/999: loss=0.1485873824251073\n",
      "SGD iter. 365/999: loss=0.14880785601983718\n",
      "SGD iter. 366/999: loss=0.14845967408159405\n",
      "SGD iter. 367/999: loss=0.14903462742207832\n",
      "SGD iter. 368/999: loss=0.1500892168674854\n",
      "SGD iter. 369/999: loss=0.1495795576908902\n",
      "SGD iter. 370/999: loss=0.1494186612293082\n",
      "SGD iter. 371/999: loss=0.1495298438771\n",
      "SGD iter. 372/999: loss=0.14955670654545386\n",
      "SGD iter. 373/999: loss=0.15093277156955592\n",
      "SGD iter. 374/999: loss=0.14732209444195496\n",
      "SGD iter. 375/999: loss=0.1471796882893481\n",
      "SGD iter. 376/999: loss=0.14661096787383493\n",
      "SGD iter. 377/999: loss=0.1469122497436772\n",
      "SGD iter. 378/999: loss=0.14759688404563348\n",
      "SGD iter. 379/999: loss=0.14696131644327484\n",
      "SGD iter. 380/999: loss=0.14676483658874726\n",
      "SGD iter. 381/999: loss=0.14654667228186954\n",
      "SGD iter. 382/999: loss=0.1460663805010736\n",
      "SGD iter. 383/999: loss=0.1466746241760546\n",
      "SGD iter. 384/999: loss=0.14702752830238403\n",
      "SGD iter. 385/999: loss=0.14726173681853613\n",
      "SGD iter. 386/999: loss=0.14754743810032034\n",
      "SGD iter. 387/999: loss=0.14791671502699502\n",
      "SGD iter. 388/999: loss=0.14761344389063824\n",
      "SGD iter. 389/999: loss=0.1466944239702899\n",
      "SGD iter. 390/999: loss=0.14863074707016083\n",
      "SGD iter. 391/999: loss=0.14807407471546216\n",
      "SGD iter. 392/999: loss=0.14824109103881644\n",
      "SGD iter. 393/999: loss=0.1484660524230041\n",
      "SGD iter. 394/999: loss=0.14824584201546998\n",
      "SGD iter. 395/999: loss=0.14742908000093705\n",
      "SGD iter. 396/999: loss=0.1472505682897966\n",
      "SGD iter. 397/999: loss=0.1472917824094083\n",
      "SGD iter. 398/999: loss=0.1438321841097645\n",
      "SGD iter. 399/999: loss=0.14407775743393558\n",
      "SGD iter. 400/999: loss=0.14420414239239332\n",
      "SGD iter. 401/999: loss=0.14477952565953706\n",
      "SGD iter. 402/999: loss=0.1449615904437153\n",
      "SGD iter. 403/999: loss=0.14473981949914522\n",
      "SGD iter. 404/999: loss=0.14501420775995413\n",
      "SGD iter. 405/999: loss=0.145292467900547\n",
      "SGD iter. 406/999: loss=0.14559693445487148\n",
      "SGD iter. 407/999: loss=0.14562735357914666\n",
      "SGD iter. 408/999: loss=0.14532037623214064\n",
      "SGD iter. 409/999: loss=0.14582310017950093\n",
      "SGD iter. 410/999: loss=0.14596173220591968\n",
      "SGD iter. 411/999: loss=0.14740585755094898\n",
      "SGD iter. 412/999: loss=0.1474383563256032\n",
      "SGD iter. 413/999: loss=0.14741092481139076\n",
      "SGD iter. 414/999: loss=0.14811258808441538\n",
      "SGD iter. 415/999: loss=0.14918400698597445\n",
      "SGD iter. 416/999: loss=0.14881233868520383\n",
      "SGD iter. 417/999: loss=0.14805084751758155\n",
      "SGD iter. 418/999: loss=0.14730319984330795\n",
      "SGD iter. 419/999: loss=0.14758641063912098\n",
      "SGD iter. 420/999: loss=0.14781877789710107\n",
      "SGD iter. 421/999: loss=0.14844093818313328\n",
      "SGD iter. 422/999: loss=0.14814534989375966\n",
      "SGD iter. 423/999: loss=0.14875268599968258\n",
      "SGD iter. 424/999: loss=0.14859473628767506\n",
      "SGD iter. 425/999: loss=0.14905898677598498\n",
      "SGD iter. 426/999: loss=0.14862104084961936\n",
      "SGD iter. 427/999: loss=0.1485600862371919\n",
      "SGD iter. 428/999: loss=0.14831757522092973\n",
      "SGD iter. 429/999: loss=0.143422030987757\n",
      "SGD iter. 430/999: loss=0.1433620655718735\n",
      "SGD iter. 431/999: loss=0.1433200499220902\n",
      "SGD iter. 432/999: loss=0.1433735835137128\n",
      "SGD iter. 433/999: loss=0.14333730406532386\n",
      "SGD iter. 434/999: loss=0.14425330084308532\n",
      "SGD iter. 435/999: loss=0.14423191423870727\n",
      "SGD iter. 436/999: loss=0.14450987414459596\n",
      "SGD iter. 437/999: loss=0.1421551941070603\n",
      "SGD iter. 438/999: loss=0.14210624419754417\n",
      "SGD iter. 439/999: loss=0.14225351118162477\n",
      "SGD iter. 440/999: loss=0.14260202534482525\n",
      "SGD iter. 441/999: loss=0.14254268263579756\n",
      "SGD iter. 442/999: loss=0.14291226846574517\n",
      "SGD iter. 443/999: loss=0.14291432468995938\n",
      "SGD iter. 444/999: loss=0.1439122973504989\n",
      "SGD iter. 445/999: loss=0.14386123082006383\n",
      "SGD iter. 446/999: loss=0.14392550370883495\n",
      "SGD iter. 447/999: loss=0.14396674838156642\n",
      "SGD iter. 448/999: loss=0.14378183098615946\n",
      "SGD iter. 449/999: loss=0.14363547518262063\n",
      "SGD iter. 450/999: loss=0.143152945932976\n",
      "SGD iter. 451/999: loss=0.14586958580652062\n",
      "SGD iter. 452/999: loss=0.14549727806722584\n",
      "SGD iter. 453/999: loss=0.14546737491519723\n",
      "SGD iter. 454/999: loss=0.1451739309355079\n",
      "SGD iter. 455/999: loss=0.14472926979903925\n",
      "SGD iter. 456/999: loss=0.14421332664035144\n",
      "SGD iter. 457/999: loss=0.14821171336019232\n",
      "SGD iter. 458/999: loss=0.14731898953297726\n",
      "SGD iter. 459/999: loss=0.15237403390564627\n",
      "SGD iter. 460/999: loss=0.1524392675625889\n",
      "SGD iter. 461/999: loss=0.14979756610500447\n",
      "SGD iter. 462/999: loss=0.1574306565575888\n",
      "SGD iter. 463/999: loss=0.15453884270935367\n",
      "SGD iter. 464/999: loss=0.1541514795267662\n",
      "SGD iter. 465/999: loss=0.15320779891116001\n",
      "SGD iter. 466/999: loss=0.15156120558610223\n",
      "SGD iter. 467/999: loss=0.15034593077098474\n",
      "SGD iter. 468/999: loss=0.15050253011122072\n",
      "SGD iter. 469/999: loss=0.15007657872112107\n",
      "SGD iter. 470/999: loss=0.1576784527940859\n",
      "SGD iter. 471/999: loss=0.15706751496815444\n",
      "SGD iter. 472/999: loss=0.1508439645696378\n",
      "SGD iter. 473/999: loss=0.14951188778418356\n",
      "SGD iter. 474/999: loss=0.15637558928837325\n",
      "SGD iter. 475/999: loss=0.15432647382611717\n",
      "SGD iter. 476/999: loss=0.15340431034894061\n",
      "SGD iter. 477/999: loss=0.15084661844002228\n",
      "SGD iter. 478/999: loss=0.15053818363038943\n",
      "SGD iter. 479/999: loss=0.14927937324694518\n",
      "SGD iter. 480/999: loss=0.14886018904076267\n",
      "SGD iter. 481/999: loss=0.14831978580117255\n",
      "SGD iter. 482/999: loss=0.14709484738190765\n",
      "SGD iter. 483/999: loss=0.14634486487009085\n",
      "SGD iter. 484/999: loss=0.15148845433305075\n",
      "SGD iter. 485/999: loss=0.1595102012510312\n",
      "SGD iter. 486/999: loss=0.15873017127021888\n",
      "SGD iter. 487/999: loss=0.1578908040177616\n",
      "SGD iter. 488/999: loss=0.15392850919507903\n",
      "SGD iter. 489/999: loss=0.15405564528781493\n",
      "SGD iter. 490/999: loss=0.15240902001587436\n",
      "SGD iter. 491/999: loss=0.15225032570141273\n",
      "SGD iter. 492/999: loss=0.15221838960362538\n",
      "SGD iter. 493/999: loss=0.1523332887187344\n",
      "SGD iter. 494/999: loss=0.15027346590275836\n",
      "SGD iter. 495/999: loss=0.14945811442101536\n",
      "SGD iter. 496/999: loss=0.1490941252524553\n",
      "SGD iter. 497/999: loss=0.14807723923361812\n",
      "SGD iter. 498/999: loss=0.14592939728616336\n",
      "SGD iter. 499/999: loss=0.1444170086114399\n",
      "SGD iter. 500/999: loss=0.1444114219265284\n",
      "SGD iter. 501/999: loss=0.14444941078129594\n",
      "SGD iter. 502/999: loss=0.14368991720426982\n",
      "SGD iter. 503/999: loss=0.1436962836588111\n",
      "SGD iter. 504/999: loss=0.14364829020039951\n",
      "SGD iter. 505/999: loss=0.14356168829187485\n",
      "SGD iter. 506/999: loss=0.14356395788526896\n",
      "SGD iter. 507/999: loss=0.14676207103467348\n",
      "SGD iter. 508/999: loss=0.14628336733512942\n",
      "SGD iter. 509/999: loss=0.1514259793789437\n",
      "SGD iter. 510/999: loss=0.15148857149050884\n",
      "SGD iter. 511/999: loss=0.1484241562877416\n",
      "SGD iter. 512/999: loss=0.1485834374786899\n",
      "SGD iter. 513/999: loss=0.1476737921492842\n",
      "SGD iter. 514/999: loss=0.14687454084686016\n",
      "SGD iter. 515/999: loss=0.15393478390615606\n",
      "SGD iter. 516/999: loss=0.15075292808041685\n",
      "SGD iter. 517/999: loss=0.1492708346096241\n",
      "SGD iter. 518/999: loss=0.14819388671078149\n",
      "SGD iter. 519/999: loss=0.14819374706413044\n",
      "SGD iter. 520/999: loss=0.1474343015888942\n",
      "SGD iter. 521/999: loss=0.14677502828010358\n",
      "SGD iter. 522/999: loss=0.14599495876366825\n",
      "SGD iter. 523/999: loss=0.14437439071894134\n",
      "SGD iter. 524/999: loss=0.14436632442666134\n",
      "SGD iter. 525/999: loss=0.1443463307435729\n",
      "SGD iter. 526/999: loss=0.14431404333327078\n",
      "SGD iter. 527/999: loss=0.14433834496443435\n",
      "SGD iter. 528/999: loss=0.14452942639334412\n",
      "SGD iter. 529/999: loss=0.14432598156924337\n",
      "SGD iter. 530/999: loss=0.14467535436409978\n",
      "SGD iter. 531/999: loss=0.14431883661596606\n",
      "SGD iter. 532/999: loss=0.1446734415215911\n",
      "SGD iter. 533/999: loss=0.14502536466657004\n",
      "SGD iter. 534/999: loss=0.14512001916365228\n",
      "SGD iter. 535/999: loss=0.1450937467364169\n",
      "SGD iter. 536/999: loss=0.1449035613764774\n",
      "SGD iter. 537/999: loss=0.14513946058655264\n",
      "SGD iter. 538/999: loss=0.14492419045809227\n",
      "SGD iter. 539/999: loss=0.14549281638513514\n",
      "SGD iter. 540/999: loss=0.14570772443784716\n",
      "SGD iter. 541/999: loss=0.14582121118263405\n",
      "SGD iter. 542/999: loss=0.1459676699735989\n",
      "SGD iter. 543/999: loss=0.14756231203881245\n",
      "SGD iter. 544/999: loss=0.14491981528719247\n",
      "SGD iter. 545/999: loss=0.1464513677595002\n",
      "SGD iter. 546/999: loss=0.14621068088522488\n",
      "SGD iter. 547/999: loss=0.14595555141309421\n",
      "SGD iter. 548/999: loss=0.1455998218722903\n",
      "SGD iter. 549/999: loss=0.14552326384619985\n",
      "SGD iter. 550/999: loss=0.14550222776400973\n",
      "SGD iter. 551/999: loss=0.14748135297397785\n",
      "SGD iter. 552/999: loss=0.1466985820836842\n",
      "SGD iter. 553/999: loss=0.15201101332611605\n",
      "SGD iter. 554/999: loss=0.1503197586059655\n",
      "SGD iter. 555/999: loss=0.1502814472455636\n",
      "SGD iter. 556/999: loss=0.1492615815380254\n",
      "SGD iter. 557/999: loss=0.14876476829723698\n",
      "SGD iter. 558/999: loss=0.1480839550870785\n",
      "SGD iter. 559/999: loss=0.1481670948897816\n",
      "SGD iter. 560/999: loss=0.1473125624913358\n",
      "SGD iter. 561/999: loss=0.14668844643036008\n",
      "SGD iter. 562/999: loss=0.14700029190734484\n",
      "SGD iter. 563/999: loss=0.14677063154983408\n",
      "SGD iter. 564/999: loss=0.14645268831231026\n",
      "SGD iter. 565/999: loss=0.14620346652792435\n",
      "SGD iter. 566/999: loss=0.14593152991126518\n",
      "SGD iter. 567/999: loss=0.14573866146706102\n",
      "SGD iter. 568/999: loss=0.14359842148610721\n",
      "SGD iter. 569/999: loss=0.14742612949071057\n",
      "SGD iter. 570/999: loss=0.14693007884958753\n",
      "SGD iter. 571/999: loss=0.1471177685593675\n",
      "SGD iter. 572/999: loss=0.14693469333604467\n",
      "SGD iter. 573/999: loss=0.14681454772801547\n",
      "SGD iter. 574/999: loss=0.1466598193422057\n",
      "SGD iter. 575/999: loss=0.14617574650360096\n",
      "SGD iter. 576/999: loss=0.1454516232979765\n",
      "SGD iter. 577/999: loss=0.14502329626224147\n",
      "SGD iter. 578/999: loss=0.14504486470779968\n",
      "SGD iter. 579/999: loss=0.14469447317265863\n",
      "SGD iter. 580/999: loss=0.144228206812851\n",
      "SGD iter. 581/999: loss=0.1442320019944011\n",
      "SGD iter. 582/999: loss=0.14423687191741819\n",
      "SGD iter. 583/999: loss=0.1439766909953417\n",
      "SGD iter. 584/999: loss=0.1441424602426025\n",
      "SGD iter. 585/999: loss=0.1446461472748551\n",
      "SGD iter. 586/999: loss=0.14485268412732977\n",
      "SGD iter. 587/999: loss=0.14609585394316416\n",
      "SGD iter. 588/999: loss=0.14595981750420814\n",
      "SGD iter. 589/999: loss=0.14556548609389766\n",
      "SGD iter. 590/999: loss=0.1453310708936949\n",
      "SGD iter. 591/999: loss=0.14496867430464783\n",
      "SGD iter. 592/999: loss=0.1450018119622986\n",
      "SGD iter. 593/999: loss=0.14488965594379707\n",
      "SGD iter. 594/999: loss=0.1450096994083361\n",
      "SGD iter. 595/999: loss=0.14486654989569653\n",
      "SGD iter. 596/999: loss=0.14477411281299837\n",
      "SGD iter. 597/999: loss=0.14414522550373227\n",
      "SGD iter. 598/999: loss=0.14639171687978558\n",
      "SGD iter. 599/999: loss=0.14627203961286447\n",
      "SGD iter. 600/999: loss=0.1462066476238628\n",
      "SGD iter. 601/999: loss=0.14645428291968632\n",
      "SGD iter. 602/999: loss=0.1464767717497535\n",
      "SGD iter. 603/999: loss=0.1503091169690524\n",
      "SGD iter. 604/999: loss=0.14918275593646896\n",
      "SGD iter. 605/999: loss=0.148726147059981\n",
      "SGD iter. 606/999: loss=0.14797857285173618\n",
      "SGD iter. 607/999: loss=0.14774910496732105\n",
      "SGD iter. 608/999: loss=0.14777206248726396\n",
      "SGD iter. 609/999: loss=0.14731379608896505\n",
      "SGD iter. 610/999: loss=0.1472886323511048\n",
      "SGD iter. 611/999: loss=0.14719718929570208\n",
      "SGD iter. 612/999: loss=0.14605895043536157\n",
      "SGD iter. 613/999: loss=0.14520628275557218\n",
      "SGD iter. 614/999: loss=0.14723442284095076\n",
      "SGD iter. 615/999: loss=0.1470699992181041\n",
      "SGD iter. 616/999: loss=0.14674783759253476\n",
      "SGD iter. 617/999: loss=0.14693478753018488\n",
      "SGD iter. 618/999: loss=0.1456583485541514\n",
      "SGD iter. 619/999: loss=0.1458605749731184\n",
      "SGD iter. 620/999: loss=0.14540599692730943\n",
      "SGD iter. 621/999: loss=0.14545107096316445\n",
      "SGD iter. 622/999: loss=0.14538927921910258\n",
      "SGD iter. 623/999: loss=0.14527804710510156\n",
      "SGD iter. 624/999: loss=0.1453705133369015\n",
      "SGD iter. 625/999: loss=0.14530698780572382\n",
      "SGD iter. 626/999: loss=0.14553767237454224\n",
      "SGD iter. 627/999: loss=0.14551930862381038\n",
      "SGD iter. 628/999: loss=0.14557592227037897\n",
      "SGD iter. 629/999: loss=0.1455148542060965\n",
      "SGD iter. 630/999: loss=0.14583769547543424\n",
      "SGD iter. 631/999: loss=0.14617465797700033\n",
      "SGD iter. 632/999: loss=0.14613857019329018\n",
      "SGD iter. 633/999: loss=0.14616553900480528\n",
      "SGD iter. 634/999: loss=0.14612221406679027\n",
      "SGD iter. 635/999: loss=0.1460041685029582\n",
      "SGD iter. 636/999: loss=0.14710395143789132\n",
      "SGD iter. 637/999: loss=0.14729116316894417\n",
      "SGD iter. 638/999: loss=0.14728007461866788\n",
      "SGD iter. 639/999: loss=0.14667959799957822\n",
      "SGD iter. 640/999: loss=0.14671214735554886\n",
      "SGD iter. 641/999: loss=0.14641802710436358\n",
      "SGD iter. 642/999: loss=0.14641329504243186\n",
      "SGD iter. 643/999: loss=0.14640693238712466\n",
      "SGD iter. 644/999: loss=0.1470052435856253\n",
      "SGD iter. 645/999: loss=0.1474082772370456\n",
      "SGD iter. 646/999: loss=0.1476191955645875\n",
      "SGD iter. 647/999: loss=0.14829724549544504\n",
      "SGD iter. 648/999: loss=0.14820344499305602\n",
      "SGD iter. 649/999: loss=0.14849359691025432\n",
      "SGD iter. 650/999: loss=0.14852398575906828\n",
      "SGD iter. 651/999: loss=0.14588875635272222\n",
      "SGD iter. 652/999: loss=0.1459221418327114\n",
      "SGD iter. 653/999: loss=0.1460784103494012\n",
      "SGD iter. 654/999: loss=0.14602516235879046\n",
      "SGD iter. 655/999: loss=0.14564663326389268\n",
      "SGD iter. 656/999: loss=0.14568972561046772\n",
      "SGD iter. 657/999: loss=0.14559018094509035\n",
      "SGD iter. 658/999: loss=0.14586553451883918\n",
      "SGD iter. 659/999: loss=0.14585389472789365\n",
      "SGD iter. 660/999: loss=0.14610955805606962\n",
      "SGD iter. 661/999: loss=0.1456587824065007\n",
      "SGD iter. 662/999: loss=0.1454840189521962\n",
      "SGD iter. 663/999: loss=0.14547554759446923\n",
      "SGD iter. 664/999: loss=0.1455041931681335\n",
      "SGD iter. 665/999: loss=0.1458733233296739\n",
      "SGD iter. 666/999: loss=0.145962302086675\n",
      "SGD iter. 667/999: loss=0.14464556911298093\n",
      "SGD iter. 668/999: loss=0.14457803914067469\n",
      "SGD iter. 669/999: loss=0.14461210182411546\n",
      "SGD iter. 670/999: loss=0.14732149279119894\n",
      "SGD iter. 671/999: loss=0.1469572443434143\n",
      "SGD iter. 672/999: loss=0.14664983257315872\n",
      "SGD iter. 673/999: loss=0.150519077772386\n",
      "SGD iter. 674/999: loss=0.15926394222125767\n",
      "SGD iter. 675/999: loss=0.15931921487325157\n",
      "SGD iter. 676/999: loss=0.15691943416508178\n",
      "SGD iter. 677/999: loss=0.15478854659582858\n",
      "SGD iter. 678/999: loss=0.1541505302800185\n",
      "SGD iter. 679/999: loss=0.15342602985244713\n",
      "SGD iter. 680/999: loss=0.15216266932992925\n",
      "SGD iter. 681/999: loss=0.15082054029785777\n",
      "SGD iter. 682/999: loss=0.15054990352217254\n",
      "SGD iter. 683/999: loss=0.15946631334652533\n",
      "SGD iter. 684/999: loss=0.1577964475094998\n",
      "SGD iter. 685/999: loss=0.15713426020406926\n",
      "SGD iter. 686/999: loss=0.15578077468948634\n",
      "SGD iter. 687/999: loss=0.1543580589464493\n",
      "SGD iter. 688/999: loss=0.15186736659081826\n",
      "SGD iter. 689/999: loss=0.1496036447427349\n",
      "SGD iter. 690/999: loss=0.15809016448257743\n",
      "SGD iter. 691/999: loss=0.15592313062178303\n",
      "SGD iter. 692/999: loss=0.15462462988997655\n",
      "SGD iter. 693/999: loss=0.15150656346231595\n",
      "SGD iter. 694/999: loss=0.1491110551487628\n",
      "SGD iter. 695/999: loss=0.14850923477365002\n",
      "SGD iter. 696/999: loss=0.14870589430002296\n",
      "SGD iter. 697/999: loss=0.14767481009101635\n",
      "SGD iter. 698/999: loss=0.1560281949959225\n",
      "SGD iter. 699/999: loss=0.15543617240991917\n",
      "SGD iter. 700/999: loss=0.15468629300044248\n",
      "SGD iter. 701/999: loss=0.15298961886087326\n",
      "SGD iter. 702/999: loss=0.15307958909144456\n",
      "SGD iter. 703/999: loss=0.15192276848316108\n",
      "SGD iter. 704/999: loss=0.15018878869367483\n",
      "SGD iter. 705/999: loss=0.15031966158075905\n",
      "SGD iter. 706/999: loss=0.1498008167472398\n",
      "SGD iter. 707/999: loss=0.14851819713790326\n",
      "SGD iter. 708/999: loss=0.1478421756004831\n",
      "SGD iter. 709/999: loss=0.14702143422107777\n",
      "SGD iter. 710/999: loss=0.14618595924003897\n",
      "SGD iter. 711/999: loss=0.15209158889479185\n",
      "SGD iter. 712/999: loss=0.15204395194960157\n",
      "SGD iter. 713/999: loss=0.1509315943446553\n",
      "SGD iter. 714/999: loss=0.15072515241119136\n",
      "SGD iter. 715/999: loss=0.15037017315884615\n",
      "SGD iter. 716/999: loss=0.1493480386574469\n",
      "SGD iter. 717/999: loss=0.14967010284578977\n",
      "SGD iter. 718/999: loss=0.1534031337106251\n",
      "SGD iter. 719/999: loss=0.15097633293457322\n",
      "SGD iter. 720/999: loss=0.15122932182429755\n",
      "SGD iter. 721/999: loss=0.1502724527882046\n",
      "SGD iter. 722/999: loss=0.15732789485951065\n",
      "SGD iter. 723/999: loss=0.15487726166547394\n",
      "SGD iter. 724/999: loss=0.1527619516761391\n",
      "SGD iter. 725/999: loss=0.1517271776894747\n",
      "SGD iter. 726/999: loss=0.1507584972597942\n",
      "SGD iter. 727/999: loss=0.149186208460252\n",
      "SGD iter. 728/999: loss=0.16118927138496414\n",
      "SGD iter. 729/999: loss=0.16149104247199272\n",
      "SGD iter. 730/999: loss=0.1562741921088481\n",
      "SGD iter. 731/999: loss=0.15417773439147028\n",
      "SGD iter. 732/999: loss=0.1621049762083699\n",
      "SGD iter. 733/999: loss=0.15934115664098367\n",
      "SGD iter. 734/999: loss=0.15692214338896646\n",
      "SGD iter. 735/999: loss=0.15646607747578234\n",
      "SGD iter. 736/999: loss=0.15574734683476688\n",
      "SGD iter. 737/999: loss=0.15503749929578128\n",
      "SGD iter. 738/999: loss=0.15507378253164045\n",
      "SGD iter. 739/999: loss=0.15437495075844684\n",
      "SGD iter. 740/999: loss=0.15919600375717904\n",
      "SGD iter. 741/999: loss=0.15761584138810178\n",
      "SGD iter. 742/999: loss=0.15770099216098046\n",
      "SGD iter. 743/999: loss=0.1573518276857286\n",
      "SGD iter. 744/999: loss=0.150490825899289\n",
      "SGD iter. 745/999: loss=0.15724013275916468\n",
      "SGD iter. 746/999: loss=0.15526600617727135\n",
      "SGD iter. 747/999: loss=0.15209505993342723\n",
      "SGD iter. 748/999: loss=0.1510236798619076\n",
      "SGD iter. 749/999: loss=0.1507292645361923\n",
      "SGD iter. 750/999: loss=0.15079763494378445\n",
      "SGD iter. 751/999: loss=0.1502740205428042\n",
      "SGD iter. 752/999: loss=0.14994801042114092\n",
      "SGD iter. 753/999: loss=0.14991187602561182\n",
      "SGD iter. 754/999: loss=0.1498845980924824\n",
      "SGD iter. 755/999: loss=0.14991536651841716\n",
      "SGD iter. 756/999: loss=0.14996108531875818\n",
      "SGD iter. 757/999: loss=0.150039804587575\n",
      "SGD iter. 758/999: loss=0.15012222946266537\n",
      "SGD iter. 759/999: loss=0.1506051541713678\n",
      "SGD iter. 760/999: loss=0.1506542634611734\n",
      "SGD iter. 761/999: loss=0.15118836968760443\n",
      "SGD iter. 762/999: loss=0.1516343623107421\n",
      "SGD iter. 763/999: loss=0.1515348633197127\n",
      "SGD iter. 764/999: loss=0.15139897697573937\n",
      "SGD iter. 765/999: loss=0.15261056263302944\n",
      "SGD iter. 766/999: loss=0.15288333276260374\n",
      "SGD iter. 767/999: loss=0.15005483771639005\n",
      "SGD iter. 768/999: loss=0.15014037106896969\n",
      "SGD iter. 769/999: loss=0.1506736819389217\n",
      "SGD iter. 770/999: loss=0.15078917949486234\n",
      "SGD iter. 771/999: loss=0.15178888115073075\n",
      "SGD iter. 772/999: loss=0.15201853304694463\n",
      "SGD iter. 773/999: loss=0.15203260670314622\n",
      "SGD iter. 774/999: loss=0.15194305574880596\n",
      "SGD iter. 775/999: loss=0.15192918423344373\n",
      "SGD iter. 776/999: loss=0.15102314647086468\n",
      "SGD iter. 777/999: loss=0.15109508118165474\n",
      "SGD iter. 778/999: loss=0.15088166254500804\n",
      "SGD iter. 779/999: loss=0.15092354164478808\n",
      "SGD iter. 780/999: loss=0.1510799791397925\n",
      "SGD iter. 781/999: loss=0.15062176898908905\n",
      "SGD iter. 782/999: loss=0.15054924294125116\n",
      "SGD iter. 783/999: loss=0.1527116690265677\n",
      "SGD iter. 784/999: loss=0.15892528915967208\n",
      "SGD iter. 785/999: loss=0.15764221400232908\n",
      "SGD iter. 786/999: loss=0.15759584166758708\n",
      "SGD iter. 787/999: loss=0.15467901138384518\n",
      "SGD iter. 788/999: loss=0.15276487282250925\n",
      "SGD iter. 789/999: loss=0.15200269427408053\n",
      "SGD iter. 790/999: loss=0.15170407735421562\n",
      "SGD iter. 791/999: loss=0.15486684578310506\n",
      "SGD iter. 792/999: loss=0.15419152047658183\n",
      "SGD iter. 793/999: loss=0.15431137743255693\n",
      "SGD iter. 794/999: loss=0.15356035676154525\n",
      "SGD iter. 795/999: loss=0.1532864321178083\n",
      "SGD iter. 796/999: loss=0.1530421217446398\n",
      "SGD iter. 797/999: loss=0.15321816885809647\n",
      "SGD iter. 798/999: loss=0.15312515726337922\n",
      "SGD iter. 799/999: loss=0.1529921960739911\n",
      "SGD iter. 800/999: loss=0.1537421847957383\n",
      "SGD iter. 801/999: loss=0.15362730903642854\n",
      "SGD iter. 802/999: loss=0.15364611529503652\n",
      "SGD iter. 803/999: loss=0.15416057922948503\n",
      "SGD iter. 804/999: loss=0.1545778227257491\n",
      "SGD iter. 805/999: loss=0.1537771523484014\n",
      "SGD iter. 806/999: loss=0.15412342394142317\n",
      "SGD iter. 807/999: loss=0.15417570237787093\n",
      "SGD iter. 808/999: loss=0.15419677529214434\n",
      "SGD iter. 809/999: loss=0.1540821386629616\n",
      "SGD iter. 810/999: loss=0.15792946247690506\n",
      "SGD iter. 811/999: loss=0.15812760155989344\n",
      "SGD iter. 812/999: loss=0.15891318601790286\n",
      "SGD iter. 813/999: loss=0.15039492240986546\n",
      "SGD iter. 814/999: loss=0.15084021765989813\n",
      "SGD iter. 815/999: loss=0.15077034453500882\n",
      "SGD iter. 816/999: loss=0.1488873557456124\n",
      "SGD iter. 817/999: loss=0.14879021683394042\n",
      "SGD iter. 818/999: loss=0.151311722622706\n",
      "SGD iter. 819/999: loss=0.15006965168789857\n",
      "SGD iter. 820/999: loss=0.15004667508905611\n",
      "SGD iter. 821/999: loss=0.14995586032367333\n",
      "SGD iter. 822/999: loss=0.14891781355171987\n",
      "SGD iter. 823/999: loss=0.14906276019003203\n",
      "SGD iter. 824/999: loss=0.14874461920048593\n",
      "SGD iter. 825/999: loss=0.14868864081211774\n",
      "SGD iter. 826/999: loss=0.14873916548724103\n",
      "SGD iter. 827/999: loss=0.14824872144773066\n",
      "SGD iter. 828/999: loss=0.14795118235566568\n",
      "SGD iter. 829/999: loss=0.14770553059417\n",
      "SGD iter. 830/999: loss=0.1477188465369272\n",
      "SGD iter. 831/999: loss=0.14771146758700313\n",
      "SGD iter. 832/999: loss=0.14761176378827595\n",
      "SGD iter. 833/999: loss=0.14759373418108365\n",
      "SGD iter. 834/999: loss=0.14759437093112135\n",
      "SGD iter. 835/999: loss=0.1476023180076702\n",
      "SGD iter. 836/999: loss=0.14671957401474714\n",
      "SGD iter. 837/999: loss=0.14675420604268344\n",
      "SGD iter. 838/999: loss=0.14663373909664246\n",
      "SGD iter. 839/999: loss=0.14663419077291417\n",
      "SGD iter. 840/999: loss=0.14661675734498697\n",
      "SGD iter. 841/999: loss=0.149560491424801\n",
      "SGD iter. 842/999: loss=0.14963664098983886\n",
      "SGD iter. 843/999: loss=0.14982726616721911\n",
      "SGD iter. 844/999: loss=0.14941258667606472\n",
      "SGD iter. 845/999: loss=0.1492514389590481\n",
      "SGD iter. 846/999: loss=0.14915625301091778\n",
      "SGD iter. 847/999: loss=0.14925489488168037\n",
      "SGD iter. 848/999: loss=0.1545712231503062\n",
      "SGD iter. 849/999: loss=0.1548603870340831\n",
      "SGD iter. 850/999: loss=0.15489853387535465\n",
      "SGD iter. 851/999: loss=0.15487664903324208\n",
      "SGD iter. 852/999: loss=0.15484386595416463\n",
      "SGD iter. 853/999: loss=0.1539685741306186\n",
      "SGD iter. 854/999: loss=0.15374168470955255\n",
      "SGD iter. 855/999: loss=0.15338656948031867\n",
      "SGD iter. 856/999: loss=0.15194965054929888\n",
      "SGD iter. 857/999: loss=0.1518827823477681\n",
      "SGD iter. 858/999: loss=0.1563490029050232\n",
      "SGD iter. 859/999: loss=0.1554121150582306\n",
      "SGD iter. 860/999: loss=0.15485692428482267\n",
      "SGD iter. 861/999: loss=0.15215035071107177\n",
      "SGD iter. 862/999: loss=0.1513578912866461\n",
      "SGD iter. 863/999: loss=0.15606287724873852\n",
      "SGD iter. 864/999: loss=0.15559144703733188\n",
      "SGD iter. 865/999: loss=0.15501778653199585\n",
      "SGD iter. 866/999: loss=0.15410557098795072\n",
      "SGD iter. 867/999: loss=0.15226229565960656\n",
      "SGD iter. 868/999: loss=0.152302822233086\n",
      "SGD iter. 869/999: loss=0.15238264171512778\n",
      "SGD iter. 870/999: loss=0.15616561627477052\n",
      "SGD iter. 871/999: loss=0.15545302806245165\n",
      "SGD iter. 872/999: loss=0.15385242357869786\n",
      "SGD iter. 873/999: loss=0.15316652353701346\n",
      "SGD iter. 874/999: loss=0.15320721611274554\n",
      "SGD iter. 875/999: loss=0.15336613536275395\n",
      "SGD iter. 876/999: loss=0.15805662411147808\n",
      "SGD iter. 877/999: loss=0.16387571843536666\n",
      "SGD iter. 878/999: loss=0.16371036257423513\n",
      "SGD iter. 879/999: loss=0.16155959812881315\n",
      "SGD iter. 880/999: loss=0.16128147792917194\n",
      "SGD iter. 881/999: loss=0.15462604530588847\n",
      "SGD iter. 882/999: loss=0.15451788676926106\n",
      "SGD iter. 883/999: loss=0.1542071328562715\n",
      "SGD iter. 884/999: loss=0.15419988524705916\n",
      "SGD iter. 885/999: loss=0.15409268327400355\n",
      "SGD iter. 886/999: loss=0.1536396915787493\n",
      "SGD iter. 887/999: loss=0.15361668492958003\n",
      "SGD iter. 888/999: loss=0.15363902892778356\n",
      "SGD iter. 889/999: loss=0.15346475412414307\n",
      "SGD iter. 890/999: loss=0.15294096567968374\n",
      "SGD iter. 891/999: loss=0.1529178644277516\n",
      "SGD iter. 892/999: loss=0.15288453927864154\n",
      "SGD iter. 893/999: loss=0.15302009699625957\n",
      "SGD iter. 894/999: loss=0.15301947870553298\n",
      "SGD iter. 895/999: loss=0.15336030573322304\n",
      "SGD iter. 896/999: loss=0.15337984283410913\n",
      "SGD iter. 897/999: loss=0.1558004343546166\n",
      "SGD iter. 898/999: loss=0.15524008277953713\n",
      "SGD iter. 899/999: loss=0.15472751665917922\n",
      "SGD iter. 900/999: loss=0.15403046852911662\n",
      "SGD iter. 901/999: loss=0.1540795268674526\n",
      "SGD iter. 902/999: loss=0.15370067318663092\n",
      "SGD iter. 903/999: loss=0.15383263809465775\n",
      "SGD iter. 904/999: loss=0.15403684773573223\n",
      "SGD iter. 905/999: loss=0.15399691754632988\n",
      "SGD iter. 906/999: loss=0.1545561776512469\n",
      "SGD iter. 907/999: loss=0.15646772666583827\n",
      "SGD iter. 908/999: loss=0.15548826868624893\n",
      "SGD iter. 909/999: loss=0.1553816901680075\n",
      "SGD iter. 910/999: loss=0.155628849676163\n",
      "SGD iter. 911/999: loss=0.15345077450979522\n",
      "SGD iter. 912/999: loss=0.15370027354665672\n",
      "SGD iter. 913/999: loss=0.15356480756144741\n",
      "SGD iter. 914/999: loss=0.15327524188259667\n",
      "SGD iter. 915/999: loss=0.15329693341583528\n",
      "SGD iter. 916/999: loss=0.1531626105142094\n",
      "SGD iter. 917/999: loss=0.15287317187369184\n",
      "SGD iter. 918/999: loss=0.1530867534925176\n",
      "SGD iter. 919/999: loss=0.15316772177773283\n",
      "SGD iter. 920/999: loss=0.15289420013880395\n",
      "SGD iter. 921/999: loss=0.15681402345520562\n",
      "SGD iter. 922/999: loss=0.15659935887217152\n",
      "SGD iter. 923/999: loss=0.156731133491918\n",
      "SGD iter. 924/999: loss=0.15592855116062593\n",
      "SGD iter. 925/999: loss=0.1556273521077553\n",
      "SGD iter. 926/999: loss=0.15556568654069045\n",
      "SGD iter. 927/999: loss=0.15527575484676423\n",
      "SGD iter. 928/999: loss=0.15528644051068857\n",
      "SGD iter. 929/999: loss=0.15496576802825643\n",
      "SGD iter. 930/999: loss=0.15741273913945067\n",
      "SGD iter. 931/999: loss=0.1549920820851164\n",
      "SGD iter. 932/999: loss=0.1549395049472713\n",
      "SGD iter. 933/999: loss=0.15494159565071047\n",
      "SGD iter. 934/999: loss=0.15505560285890282\n",
      "SGD iter. 935/999: loss=0.1552545355380692\n",
      "SGD iter. 936/999: loss=0.15514532088147778\n",
      "SGD iter. 937/999: loss=0.1562689699446277\n",
      "SGD iter. 938/999: loss=0.15651142666802562\n",
      "SGD iter. 939/999: loss=0.15610136937938868\n",
      "SGD iter. 940/999: loss=0.15602289161238142\n",
      "SGD iter. 941/999: loss=0.1554646067911665\n",
      "SGD iter. 942/999: loss=0.15547019096080456\n",
      "SGD iter. 943/999: loss=0.15479569141400518\n",
      "SGD iter. 944/999: loss=0.15473950839344597\n",
      "SGD iter. 945/999: loss=0.1538186113589689\n",
      "SGD iter. 946/999: loss=0.15341373074144993\n",
      "SGD iter. 947/999: loss=0.153417712744154\n",
      "SGD iter. 948/999: loss=0.15341032558509426\n",
      "SGD iter. 949/999: loss=0.15353545153941325\n",
      "SGD iter. 950/999: loss=0.1535870634739795\n",
      "SGD iter. 951/999: loss=0.15342949666896638\n",
      "SGD iter. 952/999: loss=0.15339407211148207\n",
      "SGD iter. 953/999: loss=0.1535596967903933\n",
      "SGD iter. 954/999: loss=0.15357143979555862\n",
      "SGD iter. 955/999: loss=0.15324187807636785\n",
      "SGD iter. 956/999: loss=0.1531339792479281\n",
      "SGD iter. 957/999: loss=0.15424102713074178\n",
      "SGD iter. 958/999: loss=0.15420443298572523\n",
      "SGD iter. 959/999: loss=0.15404706060547188\n",
      "SGD iter. 960/999: loss=0.1538837047451544\n",
      "SGD iter. 961/999: loss=0.15427805776615\n",
      "SGD iter. 962/999: loss=0.15429866674335663\n",
      "SGD iter. 963/999: loss=0.1547262490108652\n",
      "SGD iter. 964/999: loss=0.15408488331503706\n",
      "SGD iter. 965/999: loss=0.15368973691332752\n",
      "SGD iter. 966/999: loss=0.15514218425827078\n",
      "SGD iter. 967/999: loss=0.15514269686121496\n",
      "SGD iter. 968/999: loss=0.15496760215289548\n",
      "SGD iter. 969/999: loss=0.1551655939520241\n",
      "SGD iter. 970/999: loss=0.15303043352869128\n",
      "SGD iter. 971/999: loss=0.15290353972753382\n",
      "SGD iter. 972/999: loss=0.15276200596913828\n",
      "SGD iter. 973/999: loss=0.15217377487652586\n",
      "SGD iter. 974/999: loss=0.15249081783612636\n",
      "SGD iter. 975/999: loss=0.1517863120569913\n",
      "SGD iter. 976/999: loss=0.15179643213942895\n",
      "SGD iter. 977/999: loss=0.15179231498128878\n",
      "SGD iter. 978/999: loss=0.1518172075796971\n",
      "SGD iter. 979/999: loss=0.1512427577000955\n",
      "SGD iter. 980/999: loss=0.15125574121977797\n",
      "SGD iter. 981/999: loss=0.15020296020797513\n",
      "SGD iter. 982/999: loss=0.14999975330170925\n",
      "SGD iter. 983/999: loss=0.14977091928859423\n",
      "SGD iter. 984/999: loss=0.1495946291389158\n",
      "SGD iter. 985/999: loss=0.14963865340616003\n",
      "SGD iter. 986/999: loss=0.1496557346815532\n",
      "SGD iter. 987/999: loss=0.14964396161859353\n",
      "SGD iter. 988/999: loss=0.1496429419561887\n",
      "SGD iter. 989/999: loss=0.14964344336534297\n",
      "SGD iter. 990/999: loss=0.14962263077579727\n",
      "SGD iter. 991/999: loss=0.14959469405737852\n",
      "SGD iter. 992/999: loss=0.1492936227478775\n",
      "SGD iter. 993/999: loss=0.14931935500179289\n",
      "SGD iter. 994/999: loss=0.1493115500788408\n",
      "SGD iter. 995/999: loss=0.1492885149025718\n",
      "SGD iter. 996/999: loss=0.14970284591311855\n",
      "SGD iter. 997/999: loss=0.14975432686669127\n",
      "SGD iter. 998/999: loss=0.149971882921252\n",
      "SGD iter. 999/999: loss=0.1503164780419458\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(x_tr.shape[1])\n",
    "max_iters = 1000\n",
    "gamma = 0.001\n",
    "\n",
    "w_opt, loss_opt = mean_squared_error_sgd(y_tr, x_tr, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Linear regression using SGD : performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1-score:  0.10689256834809395\n",
      "\n",
      "Validation F1-score:  0.11228847066266011\n"
     ]
    }
   ],
   "source": [
    "y_pred, accuracy, precision, recall, f1_score = evaluate_performance(x_tr, y_tr, w_opt, model_labels={-1, 1}, limit=0)\n",
    "print(\"Training F1-score: \", f1_score)\n",
    "y_pred, accuracy, precision, recall, f1_score = evaluate_performance(x_val, y_val, w_opt, model_labels={-1, 1}, limit=0)\n",
    "print(\"\\nValidation F1-score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Least squares : training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_opt, loss_opt = least_squares(y_tr, x_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line would raise \"LinAlgError: Singular matrix\". Thus, we prefer to use the ridge regression to avoid the issue of singularity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Ridge regression : training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.001\n",
    "\n",
    "w_opt, loss_opt = ridge_regression(y_tr, x_tr, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Ridge regression : performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1-score:  0.09174385180453529\n",
      "\n",
      "Validation F1-score:  0.08846344281234528\n"
     ]
    }
   ],
   "source": [
    "y_pred, accuracy, precision, recall, f1_score = evaluate_performance(x_tr, y_tr, w_opt, model_labels={-1, 1}, limit=0)\n",
    "print(\"Training F1-score: \", f1_score)\n",
    "y_pred, accuracy, precision, recall, f1_score = evaluate_performance(x_val, y_val, w_opt, model_labels={-1, 1}, limit=0)\n",
    "print(\"\\nValidation F1-score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Logistic regression : training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression as we implemented it, we need the labels to fall in {0,1}:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See the different shapes : x_train (328135, 321), x_test (109379, 321), y_train (328135,), headers_train: 321\n",
      "After preprocessing (train) : column with missing values {}, are there NaN ? False\n",
      "After preprocessing (test) : column with missing values {}, are there NaN ? False\n",
      "See the different shapes : x_tr (262508, 169), x_val (65627, 169), y_tr (262508,), y_te(65627,), x_test_formatted(109379, 169)\n"
     ]
    }
   ],
   "source": [
    "x_tr, x_val, y_tr, y_val, x_train_full, x_test_formatted, remaining_headers = data_preprocess(x_train, y_train, x_test, headers_train, model_labels = {0, 1}, ratio_miss = 0.1, ratio_train = 0.8, standardization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805599448\n",
      "Current iteration=100, loss=0.23283198604893807\n",
      "Current iteration=200, loss=0.22894968466463037\n",
      "Current iteration=300, loss=0.22741513310263986\n",
      "Current iteration=400, loss=0.2265152223294939\n",
      "loss=0.22590267258575267\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(x_tr.shape[1])\n",
    "max_iters = 500\n",
    "gamma = 0.1\n",
    "\n",
    "w_opt, loss_opt = logistic_regression(y_tr, x_tr, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Logistic regression : performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1-score:  0.12020500077651809\n",
      "\n",
      "Validation F1-score:  0.1177223288547665\n"
     ]
    }
   ],
   "source": [
    "y_pred, accuracy, precision, recall, f1_score = evaluate_performance(x_tr, y_tr, w_opt, model_labels={0, 1}, limit=0.5)\n",
    "print(\"Training F1-score: \", f1_score)\n",
    "y_pred, accuracy, precision, recall, f1_score = evaluate_performance(x_val, y_val, w_opt, model_labels={0, 1}, limit=0.5)\n",
    "print(\"\\nValidation F1-score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Regularized logistic regression : training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805599448\n",
      "Current iteration=100, loss=0.2330191840909671\n",
      "Current iteration=200, loss=0.22913392346371855\n",
      "Current iteration=300, loss=0.22761390752265517\n",
      "Current iteration=400, loss=0.22673297078287427\n",
      "loss=0.22613923064778588\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.001\n",
    "initial_w = np.zeros(x_tr.shape[1])\n",
    "max_iters = 500\n",
    "gamma = 0.1\n",
    "\n",
    "w_opt, loss_opt = reg_logistic_regression(y_tr, x_tr, lambda_, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Regularized logistic regression : validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1-score:  0.11625277766948656\n",
      "\n",
      "Validation F1-score:  0.11398298282228286\n"
     ]
    }
   ],
   "source": [
    "y_pred, accuracy, precision, recall, f1_score = evaluate_performance(x_tr, y_tr, w_opt, model_labels={0, 1}, limit=0.5)\n",
    "print(\"Training F1-score: \", f1_score)\n",
    "y_pred, accuracy, precision, recall, f1_score = evaluate_performance(x_val, y_val, w_opt, model_labels={0, 1}, limit=0.5)\n",
    "print(\"\\nValidation F1-score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 A short interpretation of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_features(headers, weights, n):\n",
    "    # Get the indices of the weights sorted by absolute value in descending order\n",
    "    top_indices = np.argsort(np.abs(weights))[::-1][:n]\n",
    "    \n",
    "    # Select the top features and their corresponding weights\n",
    "    top_features = [(headers[i], weights[i]) for i in top_indices]\n",
    "    \n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_FRT16', -0.5220677880930916), ('_VEG23', -0.5211104804960296), ('_FRTRESP', -0.4489017277727273), ('_VEGRESP', -0.43253739193501856), ('_RACEG21', 0.32126095412368594)]\n"
     ]
    }
   ],
   "source": [
    "print(top_n_features(remaining_headers, w_opt, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
